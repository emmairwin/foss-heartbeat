{"original_position": 7, "diff_hunk": "@@ -62,7 +62,10 @@ this.NormandyApi = {\n   },\n \n   async getApiUrl(name) {\n-    const apiBase = prefs.getCharPref(\"api_url\");\n+    let apiBase = prefs.getCharPref(\"api_url\");\n+    if (!apiBase.endsWith('/')) {\n+      apiBase += '/';", "body_text": "If you use a URL constructor here to parse the URL, you get 2 bonuses over this approach:\n\nit'll be correct in the face of querystrings / hashes (the latter of which I imagine we can just drop anyway?) without changing those.\nit'll normalize things using our 'regular' URL parser and also allow us to formally catch invalid URLs before passing them to fetch (and throw an appropriate error).\n\nHowever, that would be slightly more code. I'm fine with either approach if we're comfortable with this butchering query strings / hashes if any occur (and still having the 301 in that case).", "in_reply_to_id": 114041817, "pull_request_url": "https://api.github.com/repos/mozilla/normandy/pulls/723", "url": "https://api.github.com/repos/mozilla/normandy/pulls/comments/114284270", "created_at": "2017-05-02T09:53:03Z", "author_association": "CONTRIBUTOR", "body": "If you use a URL constructor here to parse the URL, you get 2 bonuses over this approach:\r\n1) it'll be correct in the face of querystrings / hashes (the latter of which I imagine we can just drop anyway?) without changing those.\r\n2) it'll normalize things using our 'regular' URL parser and also allow us to formally catch invalid URLs before passing them to fetch (and throw an appropriate error).\r\n\r\nHowever, that would be slightly more code. I'm fine with either approach if we're comfortable with this butchering query strings / hashes if any occur (and still having the 301 in that case).", "updated_at": "2017-05-03T16:49:57Z", "html_url": "https://github.com/mozilla/normandy/pull/723#discussion_r114284270", "pull_request_review_id": 35735861, "_links": {"self": {"href": "https://api.github.com/repos/mozilla/normandy/pulls/comments/114284270"}, "html": {"href": "https://github.com/mozilla/normandy/pull/723#discussion_r114284270"}, "pull_request": {"href": "https://api.github.com/repos/mozilla/normandy/pulls/723"}}, "commit_id": "e570194464f5584cd819f18da24cc104b980dbaa", "user": {"following_url": "https://api.github.com/users/gijsk/following{/other_user}", "events_url": "https://api.github.com/users/gijsk/events{/privacy}", "organizations_url": "https://api.github.com/users/gijsk/orgs", "url": "https://api.github.com/users/gijsk", "gists_url": "https://api.github.com/users/gijsk/gists{/gist_id}", "html_url": "https://github.com/gijsk", "subscriptions_url": "https://api.github.com/users/gijsk/subscriptions", "avatar_url": "https://avatars3.githubusercontent.com/u/375983?v=4", "repos_url": "https://api.github.com/users/gijsk/repos", "received_events_url": "https://api.github.com/users/gijsk/received_events", "gravatar_id": "", "starred_url": "https://api.github.com/users/gijsk/starred{/owner}{/repo}", "site_admin": false, "login": "gijsk", "type": "User", "id": 375983, "followers_url": "https://api.github.com/users/gijsk/followers"}, "position": null, "path": "recipe-client-addon/lib/NormandyApi.jsm", "body_html": "<p>If you use a URL constructor here to parse the URL, you get 2 bonuses over this approach:</p>\n<ol>\n<li>it'll be correct in the face of querystrings / hashes (the latter of which I imagine we can just drop anyway?) without changing those.</li>\n<li>it'll normalize things using our 'regular' URL parser and also allow us to formally catch invalid URLs before passing them to fetch (and throw an appropriate error).</li>\n</ol>\n<p>However, that would be slightly more code. I'm fine with either approach if we're comfortable with this butchering query strings / hashes if any occur (and still having the 301 in that case).</p>", "original_commit_id": "d7801a952ad0786632c5bfece11762f41c61b6f0", "id": 114284270}