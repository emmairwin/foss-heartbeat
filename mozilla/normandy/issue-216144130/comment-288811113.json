{"body": "Your example of https://circleci.com/gh/mozilla/normandy/1892 does indeed look like the Selenium standalone server is not running when the test tries to create a new session. When starting the selenium/standalone-firefox:3.0.1-fermium container, you may want to wait for the log to contain \"Selenium Server is up and running\" or that it you can connect to it on a port mapped to 4444.\r\n\r\nFor elements not being found, looking at your page object implementation you may need to tweak your waits so that they're not only waiting for elements to be present but also visible. You may be hitting a race condition where elements exist but are not yet in a state that they can be interacted with. FYI we use a page object package named [PyPOM](https://pypi.python.org/pypi/PyPOM) to build our page objects and regions, though with a simple single page JS app you might consider this overkill.\r\n\r\nFor the waits, there are also several [expected conditions](https://github.com/SeleniumHQ/selenium/blob/master/py/selenium/webdriver/support/expected_conditions.py) provided in a support module within the Selenium package. I'm not a big fan of these, but they can be useful even if just as a model for writing your own. Sometimes you need to be specific about what you're waiting for. I also always advise an explicit wait after any interaction rather than before an interaction. For example, I click a checkbox and then wait for a dropdown to be populated.\r\n\r\nAs for intelligently retrying tests, this is possible but I would advise caution. There is a [pytest-rerunfailures](https://github.com/pytest-dev/pytest-rerunfailures) plugin that will either rerun marked flaky tests or all failing tests up to a maximum number of times. This of course will add to your test execution time, and may even mask real intermittent issues. If you do enable this I highly recommend starting with a single rerun so that you're alerted if something fails twice in a suite. My recommendation with intermittent failures is to fix them or delete them, otherwise you can lose trust in your suite and start to assume failures are just flaky tests.\r\n\r\nAdditionally, something we're doing to battle flaky tests is sending all of our test results to ActiveData. This allows us to query, analyse, and create visualisations for results. If we have a way to track intermittent failures, and the impact on suite duration when introducing or increasing automatic reruns I am much more comfortable with this feature being used. I've just seen too many cases of increasing timeouts and automatic reruns causing suites to take hours when a critical regression is introduced. Reporting to ActiveData is simply a case of publishing a raw structured log to S3. You can read more about this [here](http://davehunt.co.uk/2017/03/21/analysing-pytest-results-using-activedata.html), and it's not only limited to functional UI tests. ", "body_text": "Your example of https://circleci.com/gh/mozilla/normandy/1892 does indeed look like the Selenium standalone server is not running when the test tries to create a new session. When starting the selenium/standalone-firefox:3.0.1-fermium container, you may want to wait for the log to contain \"Selenium Server is up and running\" or that it you can connect to it on a port mapped to 4444.\nFor elements not being found, looking at your page object implementation you may need to tweak your waits so that they're not only waiting for elements to be present but also visible. You may be hitting a race condition where elements exist but are not yet in a state that they can be interacted with. FYI we use a page object package named PyPOM to build our page objects and regions, though with a simple single page JS app you might consider this overkill.\nFor the waits, there are also several expected conditions provided in a support module within the Selenium package. I'm not a big fan of these, but they can be useful even if just as a model for writing your own. Sometimes you need to be specific about what you're waiting for. I also always advise an explicit wait after any interaction rather than before an interaction. For example, I click a checkbox and then wait for a dropdown to be populated.\nAs for intelligently retrying tests, this is possible but I would advise caution. There is a pytest-rerunfailures plugin that will either rerun marked flaky tests or all failing tests up to a maximum number of times. This of course will add to your test execution time, and may even mask real intermittent issues. If you do enable this I highly recommend starting with a single rerun so that you're alerted if something fails twice in a suite. My recommendation with intermittent failures is to fix them or delete them, otherwise you can lose trust in your suite and start to assume failures are just flaky tests.\nAdditionally, something we're doing to battle flaky tests is sending all of our test results to ActiveData. This allows us to query, analyse, and create visualisations for results. If we have a way to track intermittent failures, and the impact on suite duration when introducing or increasing automatic reruns I am much more comfortable with this feature being used. I've just seen too many cases of increasing timeouts and automatic reruns causing suites to take hours when a critical regression is introduced. Reporting to ActiveData is simply a case of publishing a raw structured log to S3. You can read more about this here, and it's not only limited to functional UI tests.", "url": "https://api.github.com/repos/mozilla/normandy/issues/comments/288811113", "created_at": "2017-03-23T18:07:01Z", "author_association": "MEMBER", "html_url": "https://github.com/mozilla/normandy/issues/625#issuecomment-288811113", "updated_at": "2017-03-23T18:07:01Z", "user": {"following_url": "https://api.github.com/users/davehunt/following{/other_user}", "events_url": "https://api.github.com/users/davehunt/events{/privacy}", "organizations_url": "https://api.github.com/users/davehunt/orgs", "url": "https://api.github.com/users/davehunt", "gists_url": "https://api.github.com/users/davehunt/gists{/gist_id}", "html_url": "https://github.com/davehunt", "subscriptions_url": "https://api.github.com/users/davehunt/subscriptions", "avatar_url": "https://avatars1.githubusercontent.com/u/122800?v=4", "repos_url": "https://api.github.com/users/davehunt/repos", "received_events_url": "https://api.github.com/users/davehunt/received_events", "gravatar_id": "", "starred_url": "https://api.github.com/users/davehunt/starred{/owner}{/repo}", "site_admin": false, "login": "davehunt", "type": "User", "id": 122800, "followers_url": "https://api.github.com/users/davehunt/followers"}, "body_html": "<p>Your example of <a href=\"https://circleci.com/gh/mozilla/normandy/1892\" rel=\"nofollow\">https://circleci.com/gh/mozilla/normandy/1892</a> does indeed look like the Selenium standalone server is not running when the test tries to create a new session. When starting the selenium/standalone-firefox:3.0.1-fermium container, you may want to wait for the log to contain \"Selenium Server is up and running\" or that it you can connect to it on a port mapped to 4444.</p>\n<p>For elements not being found, looking at your page object implementation you may need to tweak your waits so that they're not only waiting for elements to be present but also visible. You may be hitting a race condition where elements exist but are not yet in a state that they can be interacted with. FYI we use a page object package named <a href=\"https://pypi.python.org/pypi/PyPOM\" rel=\"nofollow\">PyPOM</a> to build our page objects and regions, though with a simple single page JS app you might consider this overkill.</p>\n<p>For the waits, there are also several <a href=\"https://github.com/SeleniumHQ/selenium/blob/master/py/selenium/webdriver/support/expected_conditions.py\">expected conditions</a> provided in a support module within the Selenium package. I'm not a big fan of these, but they can be useful even if just as a model for writing your own. Sometimes you need to be specific about what you're waiting for. I also always advise an explicit wait after any interaction rather than before an interaction. For example, I click a checkbox and then wait for a dropdown to be populated.</p>\n<p>As for intelligently retrying tests, this is possible but I would advise caution. There is a <a href=\"https://github.com/pytest-dev/pytest-rerunfailures\">pytest-rerunfailures</a> plugin that will either rerun marked flaky tests or all failing tests up to a maximum number of times. This of course will add to your test execution time, and may even mask real intermittent issues. If you do enable this I highly recommend starting with a single rerun so that you're alerted if something fails twice in a suite. My recommendation with intermittent failures is to fix them or delete them, otherwise you can lose trust in your suite and start to assume failures are just flaky tests.</p>\n<p>Additionally, something we're doing to battle flaky tests is sending all of our test results to ActiveData. This allows us to query, analyse, and create visualisations for results. If we have a way to track intermittent failures, and the impact on suite duration when introducing or increasing automatic reruns I am much more comfortable with this feature being used. I've just seen too many cases of increasing timeouts and automatic reruns causing suites to take hours when a critical regression is introduced. Reporting to ActiveData is simply a case of publishing a raw structured log to S3. You can read more about this <a href=\"http://davehunt.co.uk/2017/03/21/analysing-pytest-results-using-activedata.html\" rel=\"nofollow\">here</a>, and it's not only limited to functional UI tests.</p>", "id": 288811113, "issue_url": "https://api.github.com/repos/mozilla/normandy/issues/625"}